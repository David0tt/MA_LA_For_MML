{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import pycalib\n",
    "from laplace import Laplace\n",
    "\n",
    "import utils.data_utils as du\n",
    "import utils.wilds_utils as wu\n",
    "import utils.utils as util\n",
    "from utils.test import test\n",
    "from marglik_training.train_marglik import get_backend\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def invImageNetNorm(x):\n",
    "    \"\"\" Inverts the Normalization given by:\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]) \"\"\"\n",
    "    invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ]),\n",
    "                               ])\n",
    "\n",
    "    return invTrans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/amazon_vanilla/'\n",
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/amazon_ts_vanilla/'\n",
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/camelyon17_vanilla/'\n",
    "\n",
    "DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/camelyon17_ts/'\n",
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/camelyon17_scaling_fitted/'\n",
    "\n",
    "\n",
    "# DATASET = 'camelyon17-id' # 'camelyon17-ood'\n",
    "DATASET = 'camelyon17-ood'\n",
    "\n",
    "x = torch.load(os.path.join(DISTRIBUTIONS_DIRECTORY, \"x_\" + DATASET + \".pt\"))\n",
    "y_true = torch.load(os.path.join(DISTRIBUTIONS_DIRECTORY, \"y_true_\" + DATASET + \".pt\"))\n",
    "y_prob = torch.load(os.path.join(DISTRIBUTIONS_DIRECTORY, \"y_prob_\" + DATASET + \".pt\"))\n",
    "\n",
    "# f_mu = torch.load(os.path.join(DISTRIBUTIONS_DIRECTORY, \"f_mu_\" + DATASET + \".pt\"))\n",
    "f_var = torch.load(os.path.join(DISTRIBUTIONS_DIRECTORY, \"f_var_\" + DATASET + \".pt\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_SUBSET = 1000\n",
    "\n",
    "# x = x[:DATA_SUBSET]\n",
    "# y_true = y_true[:DATA_SUBSET]\n",
    "# y_prob = y_prob[:, :DATA_SUBSET]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cov(points):\n",
    "    B, N, D = points.size()\n",
    "    mean = points.mean(dim=1).unsqueeze(1)\n",
    "    diffs = (points - mean).reshape(B * N, D)\n",
    "    prods = torch.bmm(diffs.unsqueeze(2), diffs.unsqueeze(1)).reshape(B, N, D, D)\n",
    "    bcov = prods.sum(dim=1) / (N - 1)  # Unbiased estimate\n",
    "    return bcov  # (B, D, D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariances = batch_cov(y_prob.permute(1,0,2))\n",
    "\n",
    "# To prevent crashing, do it in batches:\n",
    "s_list = list(range(0, y_prob.shape[1] + 10000, 5000))\n",
    "covariances = torch.cat([batch_cov(y_prob[:, start:stop].permute(1,0,2)) for start, stop in zip(s_list[:-1], s_list[1:])])\n",
    "y_pred = y_prob.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs, preds = torch.max(y_pred, 1)\n",
    "print(\"conf: \", confs.mean().item())\n",
    "print(\"acc: \", (y_true == preds).float().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = torch.tensor([c[preds[i], preds[i]] for i, c in enumerate(covariances)])\n",
    "\n",
    "print(\"mean_variance: \", variances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_variances = torch.tensor([c[preds[i], preds[i]] for i, c in enumerate(f_var)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_classified = y_true == preds\n",
    "\n",
    "wrongly_classified = torch.logical_not(correctly_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SAMPLE_IDS = torch.tensor(list(range(len(y_true))))\n",
    "\n",
    "IDS_CORRECT = ALL_SAMPLE_IDS[y_true == preds]\n",
    "IDS_WRONG = ALL_SAMPLE_IDS[torch.logical_not(y_true == preds)]\n",
    "\n",
    "IDS_HIGH_CONFIDENCE = torch.argsort(confs, descending=True)\n",
    "IDS_LOW_CONFIDENCE = torch.argsort(confs, descending=False)\n",
    "IDS_MIDDLE_CONFIDENCE = ALL_SAMPLE_IDS[torch.logical_and(confs >= 0.6, confs <= 0.7)]\n",
    "\n",
    "\n",
    "# IDS_HIGH_VARIANCE = torch.argsort(variances, descending=True)\n",
    "# IDS_LOW_VARIANCE = torch.argsort(variances, descending=False)\n",
    "\n",
    "IDS_HIGH_VARIANCE = torch.argsort(logit_variances, descending=True)\n",
    "IDS_LOW_VARIANCE = torch.argsort(logit_variances, descending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_intersection(arg1, arg2, arg3):\n",
    "    return np.intersect1d(np.intersect1d(arg1, arg2), arg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_IN_EACH_CONDITION = 30\n",
    "\n",
    "condition_dict = {}\n",
    "for correctness, correctness_name in zip([IDS_CORRECT, IDS_WRONG], [\"correct\", \"wrong\"]):\n",
    "    for confidence, confidence_name in zip([IDS_HIGH_CONFIDENCE, IDS_LOW_CONFIDENCE, IDS_MIDDLE_CONFIDENCE], [\"high_conf\", \"low_conf\", \"middle_conf\"]):\n",
    "        for variance, variance_name in zip([IDS_HIGH_VARIANCE, IDS_LOW_VARIANCE], [\"high_variance\", \"low_variance\"]):\n",
    "            \n",
    "            # Increase the number of top IDs considered for the intersection, until there are enough elements in the intersection\n",
    "            for i in range(1, len(ALL_SAMPLE_IDS), AMOUNT_IN_EACH_CONDITION):\n",
    "                condition_ids = three_intersection(correctness, confidence[:i], variance[:i])\n",
    "                if len(condition_ids) >= AMOUNT_IN_EACH_CONDITION:\n",
    "                    break\n",
    "            \n",
    "            condition_string = f'{correctness_name}_{confidence_name}_{variance_name}'\n",
    "            condition_dict[condition_string] = condition_ids[:AMOUNT_IN_EACH_CONDITION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine:\n",
    "# # Correctly classified, with high confidence\n",
    "# sort = torch.argsort(confs, descending=True)\n",
    "# ID_CORRECT_HIGH_CONF = sort[torch.nonzero(correctly_classified[sort])[0]].item()\n",
    "\n",
    "# # correctly classified with low confidence\n",
    "# sort = torch.argsort(confs, descending=False)\n",
    "# ID_CORRECT_LOW_CONF = sort[torch.nonzero(correctly_classified[sort])[0]].item()\n",
    "\n",
    "\n",
    "# # wrongly classified with high confidence\n",
    "# sort = torch.argsort(confs, descending=True)\n",
    "# ID_WRONG_HIGH_CONF = sort[torch.nonzero(wrongly_classified[sort])[0]].item()\n",
    "\n",
    "# # wrongly classified with low confidence\n",
    "# sort = torch.argsort(confs, descending=False)\n",
    "# ID_WRONG_LOW_CONF = sort[torch.nonzero(wrongly_classified[sort])[0]].item()\n",
    "\n",
    "\n",
    "# # correctly classified, with high variance in the predicted class\n",
    "# sort = torch.argsort(variances, descending=True)\n",
    "# ID_CORRECT_HIGH_VARIANCE = sort[torch.nonzero(correctly_classified[sort])[0]].item()\n",
    "\n",
    "\n",
    "# # wrongly classified, with high variance in the predicted class\n",
    "# sort = torch.argsort(variances, descending=True)\n",
    "# ID_WRONG_HIGH_VARIANCE = sort[torch.nonzero(wrongly_classified[sort])[0]].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_ids = [ID_CORRECT_HIGH_CONF, ID_CORRECT_LOW_CONF, ID_WRONG_HIGH_CONF, ID_WRONG_LOW_CONF, ID_CORRECT_HIGH_VARIANCE, ID_WRONG_HIGH_VARIANCE]\n",
    "# sample_names = [\"ID_CORRECT_HIGH_CONF\", \"ID_CORRECT_LOW_CONF\", \"ID_WRONG_HIGH_CONF\", \"ID_WRONG_LOW_CONF\", \"ID_CORRECT_HIGH_VARIANCE\", \"ID_WRONG_HIGH_VARIANCE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for SAMPLE_ID, SAMPLE_NAME in zip(sample_ids, sample_names):\n",
    "#     print(\"sample_name: \", SAMPLE_NAME)\n",
    "#     print(\"SAMPLE_ID: \", SAMPLE_ID)\n",
    "#     print(\"conf: \", confs[SAMPLE_ID])\n",
    "#     print(\"Correct: \", y_true[SAMPLE_ID] == preds[SAMPLE_ID])\n",
    "#     print(\"variance: \", variances[SAMPLE_ID])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: do all combinations of [Correct, Wrong] x [high confidence, low confidence] x [high variance, low variance]\n",
    "\n",
    "# Observe: \n",
    "#   high confidence -> no uncertainty\n",
    "#   low confidence, high variance ~= epistemic uncertainty (uncertainty is due to the randomness in the model weights)\n",
    "#   low confidence, low variance ~= aleatoric uncertainty (the model is very certain of being of low confidence, \n",
    "#                                           regardless of smal fluctuations in the weights, uncertainty due to true randomness in the training data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_name, sample_ids  in condition_dict.items():\n",
    "    sample_id = sample_ids[0].item()\n",
    "    print(\"sample_name: \", sample_name)\n",
    "    print(\"SAMPLE_ID: \", sample_id)\n",
    "    print(\"conf: \", confs[sample_id])\n",
    "    print(\"Correct: \", y_true[sample_id] == preds[sample_id])\n",
    "    print(\"variance: \", variances[sample_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just random samples: \n",
    "sample_ids = np.random.choice(ALL_SAMPLE_IDS, AMOUNT_IN_EACH_CONDITION,  replace=False)\n",
    "sample_name = \"Random Images\"\n",
    "\n",
    "fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, y_prob.shape[-1] + 1) # number of possible classes\n",
    "fig.set_size_inches(14, AMOUNT_IN_EACH_CONDITION * 1.2)\n",
    "\n",
    "for i, sample_id in enumerate(sample_ids):\n",
    "        \n",
    "    axs[i][0].imshow(invImageNetNorm(x[sample_id]).permute(1,2,0))\n",
    "    axs[i][0].set_ylabel(r'y=' + f'{y_true[sample_id].item()}; ' + r'$\\hat{y}=$' + f'{preds[sample_id]}')\n",
    "    axs[i][0].set_xticks([])\n",
    "    axs[i][0].set_yticks([])\n",
    "\n",
    "    for c in range(y_prob.shape[-1]):\n",
    "        axs[i][c+1].hist(y_prob[:,sample_id, c].numpy(), bins=20, range=(0,1))\n",
    "        axs[i][c+1].set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(f\"histogram of the confidences in each individual class\\n{sample_name}\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_name, sample_ids  in condition_dict.items():\n",
    "    fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, y_prob.shape[-1] + 1) # number of possible classes\n",
    "    fig.set_size_inches(14, AMOUNT_IN_EACH_CONDITION * 1.2)\n",
    "\n",
    "    for i, sample_id in enumerate(sample_ids):\n",
    "            \n",
    "        axs[i][0].imshow(invImageNetNorm(x[sample_id]).permute(1,2,0))\n",
    "        axs[i][0].set_ylabel(y_true[sample_id].item())\n",
    "        axs[i][0].set_xticks([])\n",
    "        axs[i][0].set_yticks([])\n",
    "\n",
    "        for c in range(y_prob.shape[-1]):\n",
    "            axs[i][c+1].hist(y_prob[:,sample_id, c].numpy(), bins=20, range=(0,1))\n",
    "            axs[i][c+1].set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "    fig.suptitle(f\"histogram of the confidences in each individual class\\n{sample_name}\", fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample_name, sample_ids  in condition_dict.items():\n",
    "#     fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, 2) # number of possible classes\n",
    "#     fig.set_size_inches(14, AMOUNT_IN_EACH_CONDITION * 1.2)\n",
    "\n",
    "#     for i, sample_id in enumerate(sample_ids):\n",
    "\n",
    "\n",
    "#         axs[i][0].imshow(invImageNetNorm(x[sample_id]).permute(1,2,0))\n",
    "#         axs[i][0].set_ylabel(y_true[sample_id].item())\n",
    "#         axs[i][0].set_xticks([])\n",
    "#         axs[i][0].set_yticks([])\n",
    "\n",
    "\n",
    "#         probs = y_prob[:, sample_id, :]\n",
    "#         m = probs.mean(dim=0)\n",
    "#         v = probs.std(dim=0)\n",
    "\n",
    "#         axs[i][1].bar(range(y_prob.shape[-1]), m, yerr=v)\n",
    "\n",
    "#         axs[i][1].set_ylim([0,1])\n",
    "\n",
    "\n",
    "\n",
    "#     fig.suptitle(f\"Posterior predictive distributions with per class variances of the confidence\\n{sample_name}\", fontsize=20)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for sample_name, sample_ids  in condition_dict.items():\n",
    "#     fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, 3) # number of possible classes\n",
    "#     fig.set_size_inches(14, AMOUNT_IN_EACH_CONDITION * 1.2)\n",
    "\n",
    "#     for i, sample_id in enumerate(sample_ids):\n",
    "#         axs[i][0].imshow(invImageNetNorm(x[sample_id]).permute(1,2,0))\n",
    "#         axs[i][0].set_ylabel(y_true[sample_id].item())\n",
    "#         axs[i][0].set_xticks([])\n",
    "#         axs[i][0].set_yticks([])\n",
    "\n",
    "\n",
    "#         probs = y_prob[:, sample_id, :]\n",
    "#         m = probs.mean(dim=0)\n",
    "#         v = probs.std(dim=0)\n",
    "\n",
    "#         axs[i][1].bar(range(y_prob.shape[-1]), m, yerr=v)\n",
    "\n",
    "#         axs[i][1].set_ylim([0,1])\n",
    "\n",
    "#         mat = axs[i][2].matshow(covariances[sample_id])\n",
    "\n",
    "#     # plt.matshow(df.corr(), fignum=fig.number)\n",
    "#     # plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n",
    "#     # plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n",
    "#     cb = plt.colorbar(mat)\n",
    "#     # cb.ax.tick_params(labelsize=14)\n",
    "\n",
    "\n",
    "\n",
    "#     fig.suptitle(f\"Posterior predictive distributions with per class variances of the confidence and covariance matrix\\n{sample_name}\", fontsize=10)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plot histogram along each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "af0c9fde4027a6d12ce721ddc579f510a33aad884d5d9cd2cd9b181ef5a6dae5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
