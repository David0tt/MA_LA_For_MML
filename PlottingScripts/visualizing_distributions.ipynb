{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import pycalib\n",
    "from laplace import Laplace\n",
    "\n",
    "import utils.data_utils as du\n",
    "import utils.wilds_utils as wu\n",
    "import utils.utils as util\n",
    "from utils.test import test\n",
    "from marglik_training.train_marglik import get_backend\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_dict = {'text.usetex': True,\n",
    "                 'font.family': 'serif',\n",
    "                 'text.latex.preamble': '\\\\renewcommand{\\\\rmdefault}{ptm}\\\\renewcommand{\\\\sfdefault}{phv}',\n",
    "                 'figure.figsize': (5.5, 3.399186938124422),\n",
    "                 'figure.constrained_layout.use': True,\n",
    "                 'figure.autolayout': False,\n",
    "                 'savefig.bbox': 'tight',\n",
    "                 'savefig.pad_inches': 0.015,\n",
    "                 'font.size': 10,\n",
    "                 'axes.labelsize': 10,\n",
    "                 'legend.fontsize': 8,\n",
    "                 'xtick.labelsize': 8,\n",
    "                 'ytick.labelsize': 8,\n",
    "                 'axes.titlesize': 10,\n",
    "                 'figure.dpi': 300}\n",
    "\n",
    "\n",
    "plt.rcParams.update(settings_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def invImageNetNorm(x):\n",
    "    \"\"\" Inverts the Normalization given by:\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]) \"\"\"\n",
    "    invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ]),\n",
    "                               ])\n",
    "\n",
    "    return invTrans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_cov(points):\n",
    "    B, N, D = points.size()\n",
    "    mean = points.mean(dim=1).unsqueeze(1)\n",
    "    diffs = (points - mean).reshape(B * N, D)\n",
    "    prods = torch.bmm(diffs.unsqueeze(2), diffs.unsqueeze(1)).reshape(B, N, D, D)\n",
    "    bcov = prods.sum(dim=1) / (N - 1)  # Unbiased estimate\n",
    "    return bcov  # (B, D, D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_samples(mean, var, n_samples, generator=None):\n",
    "    \"\"\"Produce samples from a batch of Normal distributions either parameterized\n",
    "    by a diagonal or full covariance given by `var`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : torch.Tensor\n",
    "        `(batch_size, output_dim)`\n",
    "    var : torch.Tensor\n",
    "        (co)variance of the Normal distribution\n",
    "        `(batch_size, output_dim, output_dim)` or `(batch_size, output_dim)`\n",
    "    generator : torch.Generator\n",
    "        random number generator\n",
    "    \"\"\"\n",
    "    assert mean.ndim == 2, 'Invalid input shape of mean, should be 2-dimensional.'\n",
    "    _, output_dim = mean.shape\n",
    "    randn_samples = torch.randn((output_dim, n_samples), device=mean.device, \n",
    "                                dtype=mean.dtype, generator=generator)\n",
    "    \n",
    "    if mean.shape == var.shape:\n",
    "        # diagonal covariance\n",
    "        scaled_samples = var.sqrt().unsqueeze(-1) * randn_samples.unsqueeze(0)\n",
    "        return (mean.unsqueeze(-1) + scaled_samples).permute((2, 0, 1))\n",
    "    elif mean.shape == var.shape[:2] and var.shape[-1] == mean.shape[1]:\n",
    "        # full covariance\n",
    "        scale = torch.linalg.cholesky(var)\n",
    "        scaled_samples = torch.matmul(scale, randn_samples.unsqueeze(0))  # expand batch dim\n",
    "        return (mean.unsqueeze(-1) + scaled_samples).permute((2, 0, 1))\n",
    "    else:\n",
    "        raise ValueError('Invalid input shapes.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confs_preds_variances(f_mu, f_var, y_true, n_samples = 10000, generator = None, batchsize = 128):\n",
    "    # For all images, calculate the conf and covariance\n",
    "    # To do this sample from distribution\n",
    "\n",
    "    confs_list = []\n",
    "    preds_list = []\n",
    "    variances_list = []\n",
    "\n",
    "    y_MAP_list = []\n",
    "    y_LA_list = []\n",
    "    y_prob_list = []\n",
    "\n",
    "    s_list = list(range(0, y_true.shape[0] + batchsize, batchsize))\n",
    "    # s_list = list(range(0, 1000, batchsize))\n",
    "    for start, stop in tqdm(zip(s_list[:-1], s_list[1:])):\n",
    "        f_mu_now = f_mu[start:stop]\n",
    "        f_var_now = f_var[start:stop]\n",
    "        \n",
    "        y_MAP_list.append(torch.softmax(f_mu_now, dim=-1))\n",
    "\n",
    "        f_samples = normal_samples(f_mu_now, f_var_now, n_samples, generator)\n",
    "        y_prob = torch.softmax(f_samples, dim=-1)\n",
    "\n",
    "        covariances = batch_cov(y_prob.permute(1,0,2))\n",
    "\n",
    "        y_pred = y_prob.mean(dim=0)\n",
    "\n",
    "        y_prob_list.append(y_prob)\n",
    "        y_LA_list.append(y_pred)\n",
    "\n",
    "        confs, preds = torch.max(y_pred, 1)\n",
    "\n",
    "        variances = torch.tensor([c[preds[i], preds[i]] for i, c in enumerate(covariances)])\n",
    "\n",
    "        confs_list.append(confs)\n",
    "        preds_list.append(preds)\n",
    "        variances_list.append(variances)\n",
    "\n",
    "    confs_list = torch.cat(confs_list)\n",
    "    preds_list = torch.cat(preds_list)\n",
    "    variances_list = torch.cat(variances_list)\n",
    "\n",
    "    y_MAP_list = torch.cat(y_MAP_list)\n",
    "    y_LA_list = torch.cat(y_LA_list)\n",
    "    y_prob_list = torch.cat(y_prob_list, dim=1)\n",
    "\n",
    "    return confs_list, preds_list, variances_list, y_MAP_list, y_LA_list, y_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_appropriate_testloader(dataset):\n",
    "    if dataset == 'camelyon17-id':\n",
    "        dataset = 'camelyon17'\n",
    "        train_loader, val_loader, in_test_loader = wu.get_wilds_loaders(\n",
    "            dataset, './data', 1.0, 1, download=False, use_ood_val_set=False)\n",
    "        test_loader = in_test_loader\n",
    "    elif dataset == 'camelyon17-ood':\n",
    "        dataset = 'camelyon17'\n",
    "        test_loader = wu.get_wilds_ood_test_loader(\n",
    "            dataset, './data', 1.0)\n",
    "    elif dataset == 'SkinLesions-id':\n",
    "        train_loader, val_loader, test_loader = du.get_ham10000_loaders('./data', batch_size=16, train_batch_size=16, num_workers=4, image_size=512)\n",
    "    elif dataset == 'SkinLesions-ood':\n",
    "        test_loader = du.get_SkinLesions_ood_loader(None, data_path='./data', batch_size=16, num_workers=4, image_size=512)\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(sample_ids, loader):\n",
    "    ''' Return an array with the images specified by sample_ids '''\n",
    "    images = []\n",
    "    for id in sample_ids:\n",
    "        id = int(id)\n",
    "        x = loader.dataset[id][0]\n",
    "        images.append(x.unsqueeze(0))\n",
    "    images = torch.cat(images)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(sample_id, loader):\n",
    "    return get_images([sample_id], loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/amazon_vanilla/'\n",
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/amazon_ts_vanilla/'\n",
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/camelyon17_vanilla/'\n",
    "\n",
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/camelyon17/'\n",
    "DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/camelyon17_ts/'\n",
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/camelyon17_scaling/'\n",
    "\n",
    "# DISTRIBUTIONS_DIRECTORY = './results/predictive_distributions/SkinLesions/'\n",
    "\n",
    "\n",
    "# DATASET = 'camelyon17-id' # 'camelyon17-ood'\n",
    "DATASET = 'camelyon17-ood'\n",
    "# DATASET = 'SkinLesions-ood'\n",
    "\n",
    "\n",
    "y_true = torch.load(os.path.join(DISTRIBUTIONS_DIRECTORY, \"y_true_\" + DATASET + \".pt\"))\n",
    "f_mu = torch.load(os.path.join(DISTRIBUTIONS_DIRECTORY, \"f_mu_\" + DATASET + \".pt\"))\n",
    "f_var = torch.load(os.path.join(DISTRIBUTIONS_DIRECTORY, \"f_var_\" + DATASET + \".pt\"))\n",
    "\n",
    "\n",
    "f_var = f_var * 2 # TODO remove for nicer pictures\n",
    "\n",
    "confs, preds, variances, y_MAP, y_LA, y_prob = calculate_confs_preds_variances(f_mu, f_var, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_appropriate_testloader(DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_SUBSET = 1000\n",
    "\n",
    "# x = x[:DATA_SUBSET]\n",
    "# y_true = y_true[:DATA_SUBSET]\n",
    "# y_prob = y_prob[:, :DATA_SUBSET]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # covariances = batch_cov(y_prob.permute(1,0,2))\n",
    "\n",
    "# # To prevent crashing, do it in batches:\n",
    "# s_list = list(range(0, y_prob.shape[1] + 10000, 5000))\n",
    "# covariances = torch.cat([batch_cov(y_prob[:, start:stop].permute(1,0,2)) for start, stop in zip(s_list[:-1], s_list[1:])])\n",
    "# y_pred = y_prob.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confs, preds = torch.max(y_pred, 1)\n",
    "# print(\"conf: \", confs.mean().item())\n",
    "# print(\"acc: \", (y_true == preds).float().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variances = torch.tensor([c[preds[i], preds[i]] for i, c in enumerate(covariances)])\n",
    "\n",
    "# print(\"mean_variance: \", variances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_variances = torch.tensor([c[preds[i], preds[i]] for i, c in enumerate(f_var)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_classified = y_true == preds\n",
    "\n",
    "wrongly_classified = torch.logical_not(correctly_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SAMPLE_IDS = torch.tensor(list(range(len(y_true))))\n",
    "\n",
    "IDS_CORRECT = ALL_SAMPLE_IDS[y_true == preds]\n",
    "IDS_WRONG = ALL_SAMPLE_IDS[torch.logical_not(y_true == preds)]\n",
    "\n",
    "IDS_HIGH_CONFIDENCE = torch.argsort(confs, descending=True)\n",
    "IDS_LOW_CONFIDENCE = torch.argsort(confs, descending=False)\n",
    "IDS_MIDDLE_CONFIDENCE = ALL_SAMPLE_IDS[torch.logical_and(confs >= 0.6, confs <= 0.7)]\n",
    "\n",
    "\n",
    "IDS_HIGH_VARIANCE = torch.argsort(variances, descending=True)\n",
    "IDS_LOW_VARIANCE = torch.argsort(variances, descending=False)\n",
    "\n",
    "# IDS_HIGH_VARIANCE = torch.argsort(logit_variances, descending=True)\n",
    "# IDS_LOW_VARIANCE = torch.argsort(logit_variances, descending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_intersection(arg1, arg2, arg3):\n",
    "    return np.intersect1d(np.intersect1d(arg1, arg2), arg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_IN_EACH_CONDITION = 10 # 30\n",
    "\n",
    "condition_dict = {}\n",
    "for correctness, correctness_name in zip([IDS_CORRECT, IDS_WRONG], [\"correct\", \"wrong\"]):\n",
    "    for confidence, confidence_name in zip([IDS_HIGH_CONFIDENCE, IDS_LOW_CONFIDENCE, IDS_MIDDLE_CONFIDENCE], [\"high conf\", \"low conf\", \"middle conf\"]):\n",
    "        for variance, variance_name in zip([IDS_HIGH_VARIANCE, IDS_LOW_VARIANCE], [\"high variance\", \"low variance\"]):\n",
    "            \n",
    "            # Increase the number of top IDs considered for the intersection, until there are enough elements in the intersection\n",
    "            for i in range(1, len(ALL_SAMPLE_IDS), AMOUNT_IN_EACH_CONDITION):\n",
    "                condition_ids = three_intersection(correctness, confidence[:i], variance[:i])\n",
    "                if len(condition_ids) >= AMOUNT_IN_EACH_CONDITION:\n",
    "                    break\n",
    "            \n",
    "            condition_string = f'{correctness_name} - {confidence_name} - {variance_name}'\n",
    "            condition_dict[condition_string] = condition_ids[:AMOUNT_IN_EACH_CONDITION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs_LA = torch.max(y_LA, dim=1)[0]\n",
    "confs_MAP = torch.max(y_MAP, dim=1)[0]\n",
    "conf_diffs = confs_MAP - confs_LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine:\n",
    "# # Correctly classified, with high confidence\n",
    "# sort = torch.argsort(confs, descending=True)\n",
    "# ID_CORRECT_HIGH_CONF = sort[torch.nonzero(correctly_classified[sort])[0]].item()\n",
    "\n",
    "# # correctly classified with low confidence\n",
    "# sort = torch.argsort(confs, descending=False)\n",
    "# ID_CORRECT_LOW_CONF = sort[torch.nonzero(correctly_classified[sort])[0]].item()\n",
    "\n",
    "\n",
    "# # wrongly classified with high confidence\n",
    "# sort = torch.argsort(confs, descending=True)\n",
    "# ID_WRONG_HIGH_CONF = sort[torch.nonzero(wrongly_classified[sort])[0]].item()\n",
    "\n",
    "# # wrongly classified with low confidence\n",
    "# sort = torch.argsort(confs, descending=False)\n",
    "# ID_WRONG_LOW_CONF = sort[torch.nonzero(wrongly_classified[sort])[0]].item()\n",
    "\n",
    "\n",
    "# # correctly classified, with high variance in the predicted class\n",
    "# sort = torch.argsort(variances, descending=True)\n",
    "# ID_CORRECT_HIGH_VARIANCE = sort[torch.nonzero(correctly_classified[sort])[0]].item()\n",
    "\n",
    "\n",
    "# # wrongly classified, with high variance in the predicted class\n",
    "# sort = torch.argsort(variances, descending=True)\n",
    "# ID_WRONG_HIGH_VARIANCE = sort[torch.nonzero(wrongly_classified[sort])[0]].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_ids = [ID_CORRECT_HIGH_CONF, ID_CORRECT_LOW_CONF, ID_WRONG_HIGH_CONF, ID_WRONG_LOW_CONF, ID_CORRECT_HIGH_VARIANCE, ID_WRONG_HIGH_VARIANCE]\n",
    "# sample_names = [\"ID_CORRECT_HIGH_CONF\", \"ID_CORRECT_LOW_CONF\", \"ID_WRONG_HIGH_CONF\", \"ID_WRONG_LOW_CONF\", \"ID_CORRECT_HIGH_VARIANCE\", \"ID_WRONG_HIGH_VARIANCE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for SAMPLE_ID, SAMPLE_NAME in zip(sample_ids, sample_names):\n",
    "#     print(\"sample_name: \", SAMPLE_NAME)\n",
    "#     print(\"SAMPLE_ID: \", SAMPLE_ID)\n",
    "#     print(\"conf: \", confs[SAMPLE_ID])\n",
    "#     print(\"Correct: \", y_true[SAMPLE_ID] == preds[SAMPLE_ID])\n",
    "#     print(\"variance: \", variances[SAMPLE_ID])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: do all combinations of [Correct, Wrong] x [high confidence, low confidence] x [high variance, low variance]\n",
    "\n",
    "# Observe: \n",
    "#   high confidence -> no uncertainty\n",
    "#   low confidence, high variance ~= epistemic uncertainty (uncertainty is due to the randomness in the model weights)\n",
    "#   low confidence, low variance ~= aleatoric uncertainty (the model is very certain of being of low confidence, \n",
    "#                                           regardless of smal fluctuations in the weights, uncertainty due to true randomness in the training data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample_name, sample_ids  in condition_dict.items():\n",
    "#     sample_id = sample_ids[0].item()\n",
    "#     print(\"sample_name: \", sample_name)\n",
    "#     print(\"SAMPLE_ID: \", sample_id)\n",
    "#     print(\"conf: \", confs[sample_id])\n",
    "#     print(\"Correct: \", y_true[sample_id] == preds[sample_id])\n",
    "#     print(\"variance: \", variances[sample_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples with largest conf diffs:\n",
    "_, ids = torch.sort(conf_diffs)\n",
    "sample_ids = ids[-AMOUNT_IN_EACH_CONDITION:]\n",
    "sample_ids = [sample_ids[-1], sample_ids[1]] + list(sample_ids[:-2])\n",
    "# sample_ids = ids[:AMOUNT_IN_EACH_CONDITION]\n",
    "sample_name = \"Largest conf diffs\"\n",
    "\n",
    "# sample_ids = np.random.choice(ALL_SAMPLE_IDS, AMOUNT_IN_EACH_CONDITION,  replace=False)\n",
    "# sample_name = \"Random Images\"\n",
    "\n",
    "fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, y_prob.shape[-1] + 1) # number of possible classes\n",
    "fig.set_size_inches(5.5, AMOUNT_IN_EACH_CONDITION * 0.8)\n",
    "\n",
    "for i, sample_id in enumerate(sample_ids):\n",
    "        \n",
    "    axs[i][0].imshow(invImageNetNorm(get_image(sample_id, test_loader)).permute(1,2,0))\n",
    "    # axs[i][0].set_ylabel(r'y=' + f'{y_true[sample_id].item()}; ' + r'$\\hat{y}=$' + f'{preds[sample_id]}')\n",
    "    axs[i][0].set_ylabel(y_true[sample_id].item())\n",
    "    axs[i][0].set_xticks([])\n",
    "    axs[i][0].set_yticks([])\n",
    "\n",
    "    for c in range(y_prob.shape[-1]):\n",
    "        axs[i][c+1].hist(y_prob[:,sample_id, c].numpy(), bins=20, range=(0,1))\n",
    "        axs[i][c+1].set_yticks([])\n",
    "        axs[i][c+1].axvline(y_MAP[sample_id, c].numpy(), label=r\"$\\hat{y}_{MAP}$\", color=\"red\", lw=1)\n",
    "        axs[i][c+1].axvline(y_LA[sample_id, c].numpy(), label=r\"$\\hat{y}_{LA}$\", color=\"darkblue\", lw=1)\n",
    "\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "fig.suptitle(f\"histogram of the confidences in each individual class\\n{sample_name}\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just random samples: \n",
    "sample_ids = np.random.choice(ALL_SAMPLE_IDS, AMOUNT_IN_EACH_CONDITION,  replace=False)\n",
    "sample_name = \"Random Images\"\n",
    "\n",
    "fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, y_prob.shape[-1] + 1) # number of possible classes\n",
    "fig.set_size_inches(5.5, AMOUNT_IN_EACH_CONDITION * 0.8)\n",
    "\n",
    "for i, sample_id in enumerate(sample_ids):\n",
    "        \n",
    "    axs[i][0].imshow(invImageNetNorm(get_image(sample_id, test_loader)).permute(1,2,0))\n",
    "    # axs[i][0].set_ylabel(r'y=' + f'{y_true[sample_id].item()}; ' + r'$\\hat{y}=$' + f'{preds[sample_id]}')\n",
    "    axs[i][0].set_ylabel(y_true[sample_id].item())\n",
    "    axs[i][0].set_xticks([])\n",
    "    axs[i][0].set_yticks([])\n",
    "\n",
    "    for c in range(y_prob.shape[-1]):\n",
    "        axs[i][c+1].hist(y_prob[:,sample_id, c].numpy(), bins=20, range=(0,1))\n",
    "        axs[i][c+1].set_yticks([])\n",
    "        # axs[i][c+1].axvline(y_MAP[sample_id, c].numpy(), label=r\"$\\hat{y}_{MAP}$\", color=\"red\", lw=3)\n",
    "        # axs[i][c+1].axvline(y_LA[sample_id, c].numpy(), label=r\"$\\hat{y}_{LA}$\", color=\"darkblue\", lw=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(f\"histogram of the confidences in each individual class\\n{sample_name}\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_name, sample_ids  in condition_dict.items():\n",
    "    fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, y_prob.shape[-1] + 1) # number of possible classes\n",
    "    fig.set_size_inches(5.5, AMOUNT_IN_EACH_CONDITION * 0.8)\n",
    "\n",
    "    for i, sample_id in enumerate(sample_ids):\n",
    "            \n",
    "        axs[i][0].imshow(invImageNetNorm(get_image(sample_id, test_loader)).permute(1,2,0))\n",
    "        axs[i][0].set_ylabel(y_true[sample_id].item())\n",
    "        axs[i][0].set_xticks([])\n",
    "        axs[i][0].set_yticks([])\n",
    "\n",
    "        for c in range(y_prob.shape[-1]):\n",
    "            axs[i][c+1].hist(y_prob[:,sample_id, c].numpy(), bins=20, range=(0,1))\n",
    "            axs[i][c+1].set_yticks([])\n",
    "            axs[i][c+1].axvline(y_MAP[sample_id, c].numpy(), label=r\"$\\hat{y}_{MAP}$\", color=\"red\", lw=1)\n",
    "            axs[i][c+1].axvline(y_LA[sample_id, c].numpy(), label=r\"$\\hat{y}_{LA}$\", color=\"darkblue\", lw=1)\n",
    "\n",
    "            if i==0 and c+1==2:\n",
    "                axs[i][c+1].legend()\n",
    "\n",
    "\n",
    "\n",
    "    fig.suptitle(f\"histogram of the confidences in each individual class\\n{sample_name}\", fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample_name, sample_ids  in condition_dict.items():\n",
    "#     fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, 2) # number of possible classes\n",
    "#     fig.set_size_inches(14, AMOUNT_IN_EACH_CONDITION * 1.2)\n",
    "\n",
    "#     for i, sample_id in enumerate(sample_ids):\n",
    "\n",
    "\n",
    "#         axs[i][0].imshow(invImageNetNorm(x[sample_id]).permute(1,2,0))\n",
    "#         axs[i][0].set_ylabel(y_true[sample_id].item())\n",
    "#         axs[i][0].set_xticks([])\n",
    "#         axs[i][0].set_yticks([])\n",
    "\n",
    "\n",
    "#         probs = y_prob[:, sample_id, :]\n",
    "#         m = probs.mean(dim=0)\n",
    "#         v = probs.std(dim=0)\n",
    "\n",
    "#         axs[i][1].bar(range(y_prob.shape[-1]), m, yerr=v)\n",
    "\n",
    "#         axs[i][1].set_ylim([0,1])\n",
    "\n",
    "\n",
    "\n",
    "#     fig.suptitle(f\"Posterior predictive distributions with per class variances of the confidence\\n{sample_name}\", fontsize=20)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for sample_name, sample_ids  in condition_dict.items():\n",
    "#     fig, axs = plt.subplots(AMOUNT_IN_EACH_CONDITION, 3) # number of possible classes\n",
    "#     fig.set_size_inches(14, AMOUNT_IN_EACH_CONDITION * 1.2)\n",
    "\n",
    "#     for i, sample_id in enumerate(sample_ids):\n",
    "#         axs[i][0].imshow(invImageNetNorm(x[sample_id]).permute(1,2,0))\n",
    "#         axs[i][0].set_ylabel(y_true[sample_id].item())\n",
    "#         axs[i][0].set_xticks([])\n",
    "#         axs[i][0].set_yticks([])\n",
    "\n",
    "\n",
    "#         probs = y_prob[:, sample_id, :]\n",
    "#         m = probs.mean(dim=0)\n",
    "#         v = probs.std(dim=0)\n",
    "\n",
    "#         axs[i][1].bar(range(y_prob.shape[-1]), m, yerr=v)\n",
    "\n",
    "#         axs[i][1].set_ylim([0,1])\n",
    "\n",
    "#         mat = axs[i][2].matshow(covariances[sample_id])\n",
    "\n",
    "#     # plt.matshow(df.corr(), fignum=fig.number)\n",
    "#     # plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n",
    "#     # plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n",
    "#     cb = plt.colorbar(mat)\n",
    "#     # cb.ax.tick_params(labelsize=14)\n",
    "\n",
    "\n",
    "\n",
    "#     fig.suptitle(f\"Posterior predictive distributions with per class variances of the confidence and covariance matrix\\n{sample_name}\", fontsize=10)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plot histogram along each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "af0c9fde4027a6d12ce721ddc579f510a33aad884d5d9cd2cd9b181ef5a6dae5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
