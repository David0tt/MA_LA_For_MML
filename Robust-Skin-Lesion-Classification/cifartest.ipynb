{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/well/win-fmrib-analysis/users/gqu790/conda/skylake/envs/skinclassifier/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_hip.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "import models\n",
    "# default settings\n",
    "arch = 'resnet32'\n",
    "num_classes = 10\n",
    "use_norm = False\n",
    "model = models.__dict__[arch](num_classes=num_classes, use_norm=use_norm)\n",
    "\n",
    "cols_names_classes = ['class_' + str(i) for i in range(0,num_classes)]\n",
    "cols_names_logits = ['logit_' + str(i) for i in range(0, num_classes)]\n",
    "cols_names_targets = ['target_' + str(i) for i in range(0, num_classes)]\n",
    "\n",
    "# prepare the test data.\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(79)\n",
    "test_ind = list(range(10000))\n",
    "random.shuffle(test_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ind = test_ind[:3000]\n",
    "testc_indx_1 = test_ind[3000:]\n",
    "testc_indx_2 = [x+10000 for x in test_ind[3000:]]\n",
    "testc_indx_3 = [x+20000 for x in test_ind[3000:]]\n",
    "testc_indx_4 = [x+30000 for x in test_ind[3000:]]\n",
    "testc_indx_5 = [x+40000 for x in test_ind[3000:]]\n",
    "testc_indx = testc_indx_1 + testc_indx_2 + testc_indx_3 + testc_indx_4 + testc_indx_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, sampler = torch.utils.data.SubsetRandomSampler(val_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = Image.fromarray(self.data[index])\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model parameters\n",
    "# vanilla training\n",
    "model_ckpt = './checkpoints_cls/cifar10_resnet32_CE_None_exp_0.01_0/ckpt.pth.tar'\n",
    "# ci_1\n",
    "model_ckpt_ci1 = './checkpoints_cls/cifar10_resnet32_LDAM_None_exp_0.01_0/ckpt.pth.tar'\n",
    "# ci_2\n",
    "model_ckpt_ci2 = './checkpoints_cls/cifar10_resnet32_CE_DRW_exp_0.01_0/ckpt.pth.tar'\n",
    "# rl_1\n",
    "model_ckpt_rl1 = './checkpoints_cls/cifar10_resnet32_CE_None_exp_0.01_0_cutout/ckpt.pth.tar'\n",
    "# rl_2\n",
    "model_ckpt_rl2 = './checkpoints_cls/cifar10_resnet32_CE_None_exp_0.01_0_randaugment/ckpt.pth.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10, baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "model = model.cuda(gpu)\n",
    "checkpoint = torch.load(model_ckpt, map_location = 'cuda:' + str(gpu))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarresultsdir = './cifar10results/baseline/'\n",
    "if not os.path.exists(cifarresultsdir):\n",
    "    os.makedirs(cifarresultsdir)\n",
    "\n",
    "model.eval()\n",
    "logits = []\n",
    "preds = []\n",
    "targets = []\n",
    "for i, (input, target) in enumerate(tqdm(val_loader)):\n",
    "    input = input.cuda(gpu, non_blocking = True)\n",
    "    logits_test = model(input)\n",
    "    preds_test = F.softmax(logits_test, dim = 1)\n",
    "    targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "    logits.append(logits_test.cpu().detach())\n",
    "    preds.append(preds_test.cpu().detach())\n",
    "    targets.append(targets_test)\n",
    "    \n",
    "logits = torch.cat(logits, dim=0)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "targets = torch.cat(targets, dim=0)\n",
    "    \n",
    "df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "df.to_csv(os.path.join(cifarresultsdir, 'predictions_val.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cifarcdatadir = './CIFAR-10-C/'\n",
    "\n",
    "alltestfiles = os.listdir(cifarcdatadir)\n",
    "alltestfiles.remove('labels.npy')\n",
    "\n",
    "labelcifar = np.load(cifarcdatadir + 'labels.npy')\n",
    "\n",
    "for testc in tqdm(alltestfiles):\n",
    "    \n",
    "    print('Processing corruption: ', testc[:-4])\n",
    "    \n",
    "    datasetexample = np.load(cifarcdatadir + testc)\n",
    "    \n",
    "    val_datasetc = MyDataset(datasetexample, labelcifar, transform=transform_val)\n",
    "    val_loaderc = torch.utils.data.DataLoader(val_datasetc, batch_size=1, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, sampler = torch.utils.data.SubsetRandomSampler(testc_indx))\n",
    "    \n",
    "    savefilename = 'predictions_val_' + testc[:-4] + '.csv'\n",
    "    \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for i, (input, target) in enumerate(val_loaderc):\n",
    "        input = input.cuda(gpu, non_blocking = True)\n",
    "        logits_test = model(input)\n",
    "        preds_test = F.softmax(logits_test, dim = 1)\n",
    "        targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "        logits.append(logits_test.cpu().detach())\n",
    "        preds.append(preds_test.cpu().detach())\n",
    "        targets.append(targets_test)\n",
    "\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "    df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "    df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "    df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "    df.to_csv(os.path.join(cifarresultsdir, savefilename), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10, ci1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "use_norm = True\n",
    "model = models.__dict__[arch](num_classes=num_classes, use_norm=use_norm)\n",
    "model = model.cuda(gpu)\n",
    "checkpoint = torch.load(model_ckpt_ci1, map_location = 'cuda:' + str(gpu))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarresultsdir = './cifar10results/ci1/'\n",
    "if not os.path.exists(cifarresultsdir):\n",
    "    os.makedirs(cifarresultsdir)\n",
    "\n",
    "model.eval()\n",
    "logits = []\n",
    "preds = []\n",
    "targets = []\n",
    "for i, (input, target) in enumerate(tqdm(val_loader)):\n",
    "    input = input.cuda(gpu, non_blocking = True)\n",
    "    logits_test = model(input)\n",
    "    preds_test = F.softmax(logits_test, dim = 1)\n",
    "    targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "    logits.append(logits_test.cpu().detach())\n",
    "    preds.append(preds_test.cpu().detach())\n",
    "    targets.append(targets_test)\n",
    "    \n",
    "logits = torch.cat(logits, dim=0)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "targets = torch.cat(targets, dim=0)\n",
    "    \n",
    "df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "df.to_csv(os.path.join(cifarresultsdir, 'predictions_val.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cifarcdatadir = './CIFAR-10-C/'\n",
    "\n",
    "alltestfiles = os.listdir(cifarcdatadir)\n",
    "alltestfiles.remove('labels.npy')\n",
    "\n",
    "labelcifar = np.load(cifarcdatadir + 'labels.npy')\n",
    "\n",
    "for testc in tqdm(alltestfiles):\n",
    "    \n",
    "    print('Processing corruption: ', testc[:-4])\n",
    "    \n",
    "    datasetexample = np.load(cifarcdatadir + testc)\n",
    "    \n",
    "    val_datasetc = MyDataset(datasetexample, labelcifar, transform=transform_val)\n",
    "    val_loaderc = torch.utils.data.DataLoader(val_datasetc, batch_size=1, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, sampler = torch.utils.data.SubsetRandomSampler(testc_indx))\n",
    "    \n",
    "    savefilename = 'predictions_val_' + testc[:-4] + '.csv'\n",
    "    \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for i, (input, target) in enumerate(val_loaderc):\n",
    "        input = input.cuda(gpu, non_blocking = True)\n",
    "        logits_test = model(input)\n",
    "        preds_test = F.softmax(logits_test, dim = 1)\n",
    "        targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "        logits.append(logits_test.cpu().detach())\n",
    "        preds.append(preds_test.cpu().detach())\n",
    "        targets.append(targets_test)\n",
    "\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "    df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "    df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "    df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "    df.to_csv(os.path.join(cifarresultsdir, savefilename), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10, ci2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "use_norm = False\n",
    "model = models.__dict__[arch](num_classes=num_classes, use_norm=use_norm)\n",
    "model = model.cuda(gpu)\n",
    "checkpoint = torch.load(model_ckpt_ci2, map_location = 'cuda:' + str(gpu))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarresultsdir = './cifar10results/ci2/'\n",
    "if not os.path.exists(cifarresultsdir):\n",
    "    os.makedirs(cifarresultsdir)\n",
    "\n",
    "model.eval()\n",
    "logits = []\n",
    "preds = []\n",
    "targets = []\n",
    "for i, (input, target) in enumerate(tqdm(val_loader)):\n",
    "    input = input.cuda(gpu, non_blocking = True)\n",
    "    logits_test = model(input)\n",
    "    preds_test = F.softmax(logits_test, dim = 1)\n",
    "    targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "    logits.append(logits_test.cpu().detach())\n",
    "    preds.append(preds_test.cpu().detach())\n",
    "    targets.append(targets_test)\n",
    "    \n",
    "logits = torch.cat(logits, dim=0)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "targets = torch.cat(targets, dim=0)\n",
    "    \n",
    "df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "df.to_csv(os.path.join(cifarresultsdir, 'predictions_val.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarcdatadir = './CIFAR-10-C/'\n",
    "\n",
    "alltestfiles = os.listdir(cifarcdatadir)\n",
    "alltestfiles.remove('labels.npy')\n",
    "\n",
    "labelcifar = np.load(cifarcdatadir + 'labels.npy')\n",
    "\n",
    "for testc in tqdm(alltestfiles):\n",
    "    \n",
    "    print('Processing corruption: ', testc[:-4])\n",
    "    \n",
    "    datasetexample = np.load(cifarcdatadir + testc)\n",
    "    \n",
    "    val_datasetc = MyDataset(datasetexample, labelcifar, transform=transform_val)\n",
    "    val_loaderc = torch.utils.data.DataLoader(val_datasetc, batch_size=1, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, sampler = torch.utils.data.SubsetRandomSampler(testc_indx))\n",
    "    \n",
    "    savefilename = 'predictions_val_' + testc[:-4] + '.csv'\n",
    "    \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for i, (input, target) in enumerate(val_loaderc):\n",
    "        input = input.cuda(gpu, non_blocking = True)\n",
    "        logits_test = model(input)\n",
    "        preds_test = F.softmax(logits_test, dim = 1)\n",
    "        targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "        logits.append(logits_test.cpu().detach())\n",
    "        preds.append(preds_test.cpu().detach())\n",
    "        targets.append(targets_test)\n",
    "\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "    df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "    df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "    df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "    df.to_csv(os.path.join(cifarresultsdir, savefilename), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10, rl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "model = model.cuda(gpu)\n",
    "checkpoint = torch.load(model_ckpt_rl1, map_location = 'cuda:' + str(gpu))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarresultsdir = './cifar10results/rl1/'\n",
    "if not os.path.exists(cifarresultsdir):\n",
    "    os.makedirs(cifarresultsdir)\n",
    "\n",
    "model.eval()\n",
    "logits = []\n",
    "preds = []\n",
    "targets = []\n",
    "for i, (input, target) in enumerate(tqdm(val_loader)):\n",
    "    input = input.cuda(gpu, non_blocking = True)\n",
    "    logits_test = model(input)\n",
    "    preds_test = F.softmax(logits_test, dim = 1)\n",
    "    targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "    logits.append(logits_test.cpu().detach())\n",
    "    preds.append(preds_test.cpu().detach())\n",
    "    targets.append(targets_test)\n",
    "    \n",
    "logits = torch.cat(logits, dim=0)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "targets = torch.cat(targets, dim=0)\n",
    "    \n",
    "df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "df.to_csv(os.path.join(cifarresultsdir, 'predictions_val.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarcdatadir = './CIFAR-10-C/'\n",
    "\n",
    "alltestfiles = os.listdir(cifarcdatadir)\n",
    "alltestfiles.remove('labels.npy')\n",
    "\n",
    "labelcifar = np.load(cifarcdatadir + 'labels.npy')\n",
    "\n",
    "for testc in tqdm(alltestfiles):\n",
    "    \n",
    "    print('Processing corruption: ', testc[:-4])\n",
    "    \n",
    "    datasetexample = np.load(cifarcdatadir + testc)\n",
    "    \n",
    "    val_datasetc = MyDataset(datasetexample, labelcifar, transform=transform_val)\n",
    "    val_loaderc = torch.utils.data.DataLoader(val_datasetc, batch_size=1, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, sampler = torch.utils.data.SubsetRandomSampler(testc_indx))\n",
    "    \n",
    "    savefilename = 'predictions_val_' + testc[:-4] + '.csv'\n",
    "    \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for i, (input, target) in enumerate(val_loaderc):\n",
    "        input = input.cuda(gpu, non_blocking = True)\n",
    "        logits_test = model(input)\n",
    "        preds_test = F.softmax(logits_test, dim = 1)\n",
    "        targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "        logits.append(logits_test.cpu().detach())\n",
    "        preds.append(preds_test.cpu().detach())\n",
    "        targets.append(targets_test)\n",
    "\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "    df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "    df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "    df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "    df.to_csv(os.path.join(cifarresultsdir, savefilename), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10, rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "model = model.cuda(gpu)\n",
    "checkpoint = torch.load(model_ckpt_rl2, map_location = 'cuda:' + str(gpu))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarresultsdir = './cifar10results/rl2/'\n",
    "if not os.path.exists(cifarresultsdir):\n",
    "    os.makedirs(cifarresultsdir)\n",
    "\n",
    "model.eval()\n",
    "logits = []\n",
    "preds = []\n",
    "targets = []\n",
    "for i, (input, target) in enumerate(tqdm(val_loader)):\n",
    "    input = input.cuda(gpu, non_blocking = True)\n",
    "    logits_test = model(input)\n",
    "    preds_test = F.softmax(logits_test, dim = 1)\n",
    "    targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "    logits.append(logits_test.cpu().detach())\n",
    "    preds.append(preds_test.cpu().detach())\n",
    "    targets.append(targets_test)\n",
    "    \n",
    "logits = torch.cat(logits, dim=0)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "targets = torch.cat(targets, dim=0)\n",
    "    \n",
    "df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "df.to_csv(os.path.join(cifarresultsdir, 'predictions_val.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifarcdatadir = './CIFAR-10-C/'\n",
    "\n",
    "alltestfiles = os.listdir(cifarcdatadir)\n",
    "alltestfiles.remove('labels.npy')\n",
    "\n",
    "labelcifar = np.load(cifarcdatadir + 'labels.npy')\n",
    "\n",
    "for testc in tqdm(alltestfiles):\n",
    "    \n",
    "    print('Processing corruption: ', testc[:-4])\n",
    "    \n",
    "    datasetexample = np.load(cifarcdatadir + testc)\n",
    "    \n",
    "    val_datasetc = MyDataset(datasetexample, labelcifar, transform=transform_val)\n",
    "    val_loaderc = torch.utils.data.DataLoader(val_datasetc, batch_size=1, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, sampler = torch.utils.data.SubsetRandomSampler(testc_indx))\n",
    "    \n",
    "    savefilename = 'predictions_val_' + testc[:-4] + '.csv'\n",
    "    \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for i, (input, target) in enumerate(val_loaderc):\n",
    "        input = input.cuda(gpu, non_blocking = True)\n",
    "        logits_test = model(input)\n",
    "        preds_test = F.softmax(logits_test, dim = 1)\n",
    "        targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "        logits.append(logits_test.cpu().detach())\n",
    "        preds.append(preds_test.cpu().detach())\n",
    "        targets.append(targets_test)\n",
    "\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "    df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "    df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "    df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "    df.to_csv(os.path.join(cifarresultsdir, savefilename), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model parameters\n",
    "# vanilla training\n",
    "model_ckpt = './checkpoints_cls/cifar100_resnet32_CE_None_exp_0.01_0/ckpt.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "import models\n",
    "# default settings\n",
    "arch = 'resnet32'\n",
    "num_classes = 100\n",
    "use_norm = False\n",
    "model = models.__dict__[arch](num_classes=num_classes, use_norm=use_norm)\n",
    "\n",
    "cols_names_classes = ['class_' + str(i) for i in range(0,num_classes)]\n",
    "cols_names_logits = ['logit_' + str(i) for i in range(0, num_classes)]\n",
    "cols_names_targets = ['target_' + str(i) for i in range(0, num_classes)]\n",
    "\n",
    "# prepare the test data.\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = 0\n",
    "model = model.cuda(gpu)\n",
    "checkpoint = torch.load(model_ckpt, map_location = 'cuda:' + str(gpu))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 3000/3000 [00:18<00:00, 160.27it/s]\n"
     ]
    }
   ],
   "source": [
    "cifarresultsdir = './cifar100results/baseline/'\n",
    "if not os.path.exists(cifarresultsdir):\n",
    "    os.makedirs(cifarresultsdir)\n",
    "\n",
    "model.eval()\n",
    "logits = []\n",
    "preds = []\n",
    "targets = []\n",
    "for i, (input, target) in enumerate(tqdm(val_loader)):\n",
    "    input = input.cuda(gpu, non_blocking = True)\n",
    "    logits_test = model(input)\n",
    "    preds_test = F.softmax(logits_test, dim = 1)\n",
    "    targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "    logits.append(logits_test.cpu().detach())\n",
    "    preds.append(preds_test.cpu().detach())\n",
    "    targets.append(targets_test)\n",
    "    \n",
    "logits = torch.cat(logits, dim=0)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "targets = torch.cat(targets, dim=0)\n",
    "    \n",
    "df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "df.to_csv(os.path.join(cifarresultsdir, 'predictions_val.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                         | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  gaussian_blur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▍                                           | 1/19 [03:25<1:01:34, 205.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  motion_blur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████                                           | 2/19 [06:52<58:26, 206.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  snow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████▌                                        | 3/19 [10:18<55:03, 206.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  contrast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██████████                                      | 4/19 [13:47<51:51, 207.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  jpeg_compression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████▋                                   | 5/19 [17:11<48:06, 206.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  shot_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████▏                                | 6/19 [20:36<44:35, 205.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  saturate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████▋                              | 7/19 [24:03<41:12, 206.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  impulse_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████▏                           | 8/19 [27:27<37:39, 205.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  pixelate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████▋                         | 9/19 [30:53<34:15, 205.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  speckle_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████████▋                      | 10/19 [34:18<30:47, 205.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  frost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████▏                   | 11/19 [37:42<27:19, 204.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  defocus_blur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████▋                 | 12/19 [41:06<23:53, 204.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  brightness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████▏              | 13/19 [44:30<20:26, 204.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  gaussian_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████▋            | 14/19 [47:52<16:59, 203.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  zoom_blur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████          | 15/19 [51:18<13:38, 204.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  fog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████▌       | 16/19 [54:40<10:10, 203.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  spatter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████████████     | 17/19 [58:08<06:50, 205.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  glass_blur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████▋  | 18/19 [1:01:32<03:24, 204.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corruption:  elastic_transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 19/19 [1:04:56<00:00, 205.07s/it]\n"
     ]
    }
   ],
   "source": [
    "cifarcdatadir = './CIFAR-100-C/'\n",
    "\n",
    "alltestfiles = os.listdir(cifarcdatadir)\n",
    "alltestfiles.remove('labels.npy')\n",
    "alltestfiles.remove('README.txt')\n",
    "\n",
    "labelcifar = np.load(cifarcdatadir + 'labels.npy')\n",
    "\n",
    "for testc in tqdm(alltestfiles):\n",
    "    \n",
    "    print('Processing corruption: ', testc[:-4])\n",
    "    \n",
    "    datasetexample = np.load(cifarcdatadir + testc)\n",
    "    \n",
    "    val_datasetc = MyDataset(datasetexample, labelcifar, transform=transform_val)\n",
    "    val_loaderc = torch.utils.data.DataLoader(val_datasetc, batch_size=1, shuffle=False,\n",
    "        num_workers=4, pin_memory=True, sampler = torch.utils.data.SubsetRandomSampler(testc_indx))\n",
    "    \n",
    "    savefilename = 'predictions_val_' + testc[:-4] + '.csv'\n",
    "    \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for i, (input, target) in enumerate(val_loaderc):\n",
    "        input = input.cuda(gpu, non_blocking = True)\n",
    "        logits_test = model(input)\n",
    "        preds_test = F.softmax(logits_test, dim = 1)\n",
    "        targets_test = F.one_hot(target, num_classes = num_classes)\n",
    "        logits.append(logits_test.cpu().detach())\n",
    "        preds.append(preds_test.cpu().detach())\n",
    "        targets.append(targets_test)\n",
    "\n",
    "    logits = torch.cat(logits, dim=0)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    df = pd.DataFrame(data=preds.numpy(), columns=cols_names_classes)\n",
    "    df_logits = pd.DataFrame(data=logits.numpy(), columns=cols_names_logits)\n",
    "    df_targets = pd.DataFrame(data=targets.numpy(), columns=cols_names_targets)\n",
    "    df = pd.concat([df, df_logits, df_targets], axis=1)\n",
    "    df.to_csv(os.path.join(cifarresultsdir, savefilename), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skinclassifier",
   "language": "python",
   "name": "skinclassifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
